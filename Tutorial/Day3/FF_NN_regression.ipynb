{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "cyLbgG6tItVp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Model definition\n",
        "# ----------------------------\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple feedforward neural network:\n",
        "    - Input: 28x28 image (flattened to 784 features)\n",
        "    - Hidden layers: two fully connected layers with ReLU activations\n",
        "    - Output: a single scalar (regression target)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 512),  # first hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),      # second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),        # final layer outputs 1 value\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)           # flatten input from [B, 1, 28, 28] -> [B, 784]\n",
        "        return self.linear_relu_stack(x)"
      ],
      "metadata": {
        "id": "lEHun9QMIx2H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iqRXnYi0Ihpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Ground-truth function (synthetic regression target)\n",
        "# ----------------------------\n",
        "INPUT_DIM = 28 * 28\n",
        "torch.manual_seed(0)  # for reproducibility\n",
        "\n",
        "# Hidden \"true\" linear mapping: y = xW + b\n",
        "W_target = torch.randn(INPUT_DIM, 1) * 0.5\n",
        "b_target = torch.randn(1) * 0.5\n",
        "\n",
        "\n",
        "def f(x_flat):\n",
        "    \"\"\"\n",
        "    Ground-truth function for regression.\n",
        "    Args:\n",
        "        x_flat: tensor of shape [batch, 784]\n",
        "    Returns:\n",
        "        tensor of shape [batch, 1]\n",
        "    \"\"\"\n",
        "    return x_flat.mm(W_target) + b_target\n",
        "\n",
        "\n",
        "def make_dataset(n_samples=60000):\n",
        "    \"\"\"\n",
        "    Generate a synthetic dataset of (x, y) pairs.\n",
        "    - x: random \"image-like\" inputs of shape [N, 1, 28, 28]\n",
        "    - y: regression targets computed from ground-truth function f\n",
        "    \"\"\"\n",
        "    x = torch.randn(n_samples, 1, 28, 28)\n",
        "    x_flat = x.view(n_samples, -1)\n",
        "    y = f(x_flat)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_split_indices(n_samples, train_ratio=0.7, val_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Compute number of samples in train/val/test split.\n",
        "    Args:\n",
        "        n_samples: total number of samples\n",
        "        train_ratio: fraction of training samples\n",
        "        val_ratio: fraction of validation samples\n",
        "    Returns:\n",
        "        (n_train, n_val, n_test)\n",
        "    \"\"\"\n",
        "    n_train = int(n_samples * train_ratio)\n",
        "    n_val = int(n_samples * val_ratio)\n",
        "    n_test = n_samples - n_train - n_val\n",
        "    return n_train, n_val, n_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select device: CUDA > MPS (Apple Silicon) > CPU\n",
        "device = (\n",
        "        torch.device(\"cuda\")\n",
        "        if torch.cuda.is_available()\n",
        "        else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
        "        else torch.device(\"cpu\"))\n",
        "\n",
        "# Generate dataset and split into train/val/test\n",
        "N = 60000\n",
        "X, Y = make_dataset(N)\n",
        "n_train, n_val, n_test = get_split_indices(N)\n",
        "\n",
        "X_train, Y_train = X[:n_train], Y[:n_train]\n",
        "X_val, Y_val = X[n_train:n_train+n_val], Y[n_train:n_train+n_val]\n",
        "X_test, Y_test = X[n_train+n_val:], Y[n_train+n_val:]\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = NeuralNetwork().to(device)\n",
        "loss_fn = nn.MSELoss()  # regression loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "# Training settings\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "HX4QaDFSI4xi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training loop\n",
        "# ----------------------------\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "        # Shuffle training set each epoch\n",
        "        perm = torch.randperm(n_train)\n",
        "        X_train = X_train[perm]\n",
        "        Y_train = Y_train[perm]\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Mini-batch training\n",
        "        for i in range(0, n_train, BATCH_SIZE):\n",
        "            x_batch = X_train[i:i+BATCH_SIZE].to(device)\n",
        "            y_batch = Y_train[i:i+BATCH_SIZE].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(x_batch)\n",
        "            loss = loss_fn(pred, y_batch)\n",
        "\n",
        "            # Backward pass + parameter update\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate weighted batch loss\n",
        "            total_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        # Average training loss\n",
        "        avg_train_loss = total_loss / n_train\n",
        "\n",
        "        # ----------------------------\n",
        "        # Validation phase\n",
        "        # ----------------------------\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_val = model(X_val.to(device))\n",
        "            val_loss = loss_fn(pred_val, Y_val.to(device)).item()\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}: train_loss={avg_train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Final test evaluation\n",
        "# ----------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "        pred_test = model(X_test.to(device))\n",
        "        test_loss = loss_fn(pred_test, Y_test.to(device)).item()\n",
        "print(f\"Test MSE: {test_loss:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SweogmeyJN8v",
        "outputId": "36cf91fe-f0cd-4b66-f699-a2db2dbdd946"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: train_loss=48.364317, val_loss=17.835579\n",
            "Epoch 02: train_loss=28.789821, val_loss=9.167013\n",
            "Epoch 03: train_loss=16.698547, val_loss=39.731724\n",
            "Epoch 04: train_loss=15.485734, val_loss=5.940180\n",
            "Epoch 05: train_loss=10.399819, val_loss=8.941078\n",
            "Epoch 06: train_loss=9.359571, val_loss=3.547918\n",
            "Epoch 07: train_loss=5.446788, val_loss=5.587064\n",
            "Epoch 08: train_loss=5.339022, val_loss=2.952600\n",
            "Epoch 09: train_loss=4.955244, val_loss=3.221295\n",
            "Epoch 10: train_loss=3.831637, val_loss=2.169223\n",
            "Test MSE: 2.173250\n"
          ]
        }
      ]
    }
  ]
}