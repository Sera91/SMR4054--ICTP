{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJWIWDc3zfAh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJXWRYaAU_OM",
        "outputId": "041a7a8f-4d48-4bf2-f6e5-0f669150c0e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Download the required packages that we will use for this tutorial\n",
        "\n",
        "!pip install netCDF4\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.8.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2.0.2)\n",
            "Downloading netCDF4-1.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4.post1 netCDF4-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jEljkaC7NC9"
      },
      "source": [
        "%matplotlib inline\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcAlTvD917Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the data with wget\n",
        "!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.1880to2018.nc\n",
        "!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/nino34.long.anom.data.txt"
      ],
      "metadata": {
        "id": "uISWcNSy-Cie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdb85ba-b0eb-40dd-eac1-feab9257e7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-09 22:17:56--  http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.1880to2018.nc\n",
            "Resolving portal.nersc.gov (portal.nersc.gov)... 128.55.206.111, 128.55.206.113, 128.55.206.109, ...\n",
            "Connecting to portal.nersc.gov (portal.nersc.gov)|128.55.206.111|:80... connected.\n",
            "HTTP request sent, awaiting response... 308 Permanent Redirect\n",
            "Location: https://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.1880to2018.nc [following]\n",
            "--2025-09-09 22:17:56--  https://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.1880to2018.nc\n",
            "Connecting to portal.nersc.gov (portal.nersc.gov)|128.55.206.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 432481041 (412M) [application/x-netcdf]\n",
            "Saving to: ‘sst.mon.mean.trefadj.anom.1880to2018.nc’\n",
            "\n",
            "sst.mon.mean.trefad 100%[===================>] 412.45M  23.8MB/s    in 18s     \n",
            "\n",
            "2025-09-09 22:18:15 (22.5 MB/s) - ‘sst.mon.mean.trefadj.anom.1880to2018.nc’ saved [432481041/432481041]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2025-09-09 22:18:15--  https://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/nino34.long.anom.data.txt\n",
            "Resolving portal.nersc.gov (portal.nersc.gov)... 128.55.206.111, 128.55.206.113, 128.55.206.109, ...\n",
            "Connecting to portal.nersc.gov (portal.nersc.gov)|128.55.206.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15449 (15K) [text/plain]\n",
            "Saving to: ‘nino34.long.anom.data.txt’\n",
            "\n",
            "nino34.long.anom.da 100%[===================>]  15.09K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-09-09 22:18:15 (195 KB/s) - ‘nino34.long.anom.data.txt’ saved [15449/15449]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the data"
      ],
      "metadata": {
        "id": "P5OyLTk51i3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    air_temp.isel(time=0).plot() # Select a single time step for a 2D map\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rYSW3PCHzua_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "\n",
        "def load_enso_indices():\n",
        "  \"\"\"\n",
        "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "\n",
        "    pd.Series : monthly ENSO values starting from 1870-01-01\n",
        "  \"\"\"\n",
        "  with open('nino34.long.anom.data.txt') as f:\n",
        "    line = f.readline()\n",
        "    enso_vals = []\n",
        "    while line:\n",
        "        yearly_enso_vals = map(float, line.split()[1:])\n",
        "        enso_vals.extend(yearly_enso_vals)\n",
        "        line = f.readline()\n",
        "\n",
        "  enso_vals = pd.Series(enso_vals)\n",
        "  enso_vals.index = pd.date_range('1870-01-01',freq='MS',\n",
        "                                  periods=len(enso_vals))\n",
        "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
        "  return enso_vals\n",
        "\n",
        "def assemble_predictors_predictands(start_date, end_date, lead_time,\n",
        "                                    dataset, data_format,\n",
        "                                    num_input_time_steps=1,\n",
        "                                    use_pca=False, n_components=32,\n",
        "                                    lat_slice=None, lon_slice=None):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date\n",
        "      lead_time            str : the number of months between each sst\n",
        "                              value and the target Nino3.4 Index\n",
        "      dataset              str : 'observations' 'CNRM' or 'MPI'\n",
        "      data_format          str : 'spatial' or 'flatten'. 'spatial' preserves\n",
        "                                  the lat/lon dimensions and returns an\n",
        "                                  array of shape (num_samples, num_input_time_steps,\n",
        "                                  lat, lon).  'flatten' returns an array of shape\n",
        "                                  (num_samples, num_input_time_steps*lat*lon)\n",
        "      num_input_time_steps int : the number of time steps to use for each\n",
        "                                 predictor sample\n",
        "      use_pca             bool : whether or not to apply principal components\n",
        "                              analysis to the sst field\n",
        "      n_components         int : the number of components to use for PCA\n",
        "      lat_slice           slice: the slice of latitudes to use\n",
        "      lon_slice           slice: the slice of longitudes to use\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of sst temperature anomalies)\n",
        "      and the predictands (np array the ENSO index at the specified lead time).\n",
        "\n",
        "  \"\"\"\n",
        "  file_name = {'observations' : 'sst.mon.mean.trefadj.anom.1880to2018.nc',\n",
        "               'CNRM'         : 'CNRM_tas_anomalies_regridded.nc',\n",
        "               'MPI'          : 'MPI_tas_anomalies_regridded.nc'}[dataset]\n",
        "  variable_name = {'observations' : 'sst',\n",
        "                   'CNRM'         : 'tas',\n",
        "                   'MPI'          : 'tas'}[dataset]\n",
        "  ds = xr.open_dataset(file_name)\n",
        "  sst = ds[variable_name].sel(time=slice(start_date, end_date))\n",
        "  if lat_slice is not None:\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    raise NotImplementedError(\"If you desire, you must implement the slicing!\")\n",
        "  if lon_slice is not None:\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    raise NotImplementedError(\"If you desire, you must implement the slicing!\")\n",
        "\n",
        "\n",
        "  num_samples = sst.shape[0]\n",
        "  #sst is a (num_samples, lat, lon) array\n",
        "  #the line below converts it to (num_samples, num_input_time_steps, lat, lon)\n",
        "  sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
        "                                                              num_samples+1)])\n",
        "  #CHALLENGE: CAN YOU IMPLEMENT THE ABOVE LINE WITHOUT A FOR LOOP?\n",
        "  num_samples = sst.shape[0]\n",
        "\n",
        "  sst[np.isnan(sst)] = 0\n",
        "  if data_format=='flatten':\n",
        "    #sst is a 3D array: (time_steps, lat, lon)\n",
        "    #in this tutorial, we will not be using ML models that take\n",
        "    #advantage of the spatial nature of global temperature\n",
        "    #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
        "    #(At each time step, there are lat*lon predictors)\n",
        "    sst = sst.reshape(num_samples, -1)\n",
        "\n",
        "\n",
        "    #Use Principal Components Analysis, also called\n",
        "    #Empirical Orthogonal Functions, to reduce the\n",
        "    #dimensionality of the array\n",
        "    if use_pca:\n",
        "      pca = sklearn.decomposition.PCA(n_components=n_components)\n",
        "      pca.fit(sst)\n",
        "      X = pca.transform(sst)\n",
        "    else:\n",
        "      X = sst\n",
        "  else: # data_format=='spatial'\n",
        "    X = sst\n",
        "\n",
        "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
        "                        pd.DateOffset(months=lead_time+num_input_time_steps-1)\n",
        "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
        "                      pd.DateOffset(months=lead_time)\n",
        "  if dataset == 'observations':\n",
        "    y = load_enso_indices()[slice(start_date_plus_lead,\n",
        "                                  end_date_plus_lead)]\n",
        "  else: #the data is from a GCM\n",
        "    X = X.astype(np.float32)\n",
        "    #The Nino3.4 Index is composed of three month rolling values\n",
        "    #Therefore, when calculating the Nino3.4 Index in a GCM\n",
        "    #we have to extract the two months prior to the first target start date\n",
        "    target_start_date_with_2_month = start_date_plus_lead - pd.DateOffset(months=2)\n",
        "    subsetted_ds = ds[variable_name].sel(time=slice(target_start_date_with_2_month,\n",
        "                                                   end_date_plus_lead))\n",
        "    #Calculate the Nino3.4 index\n",
        "    y = subsetted_ds.sel(lat=slice(5,-5), lon=slice(360-170,360-120)).mean(dim=('lat','lon'))\n",
        "\n",
        "    y = pd.Series(y.values).rolling(window=3).mean()[2:].values\n",
        "    y = y.astype(np.float32)\n",
        "  ds.close()\n",
        "  return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "\n",
        "class ENSODataset(Dataset):\n",
        "    def __init__(self, predictors, predictands):\n",
        "        self.predictors = predictors\n",
        "        self.predictands = predictands\n",
        "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
        "               \"The number of predictors must equal the number of predictands!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.predictors.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.predictands[idx]"
      ],
      "metadata": {
        "id": "yN8IEjwVzsbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train a simple CNN to forecast ENSO\n",
        "\n",
        "Let's define a simple convolutional neural network architecture. This architecture has 1 convolutional layer, followed by a pooling layer, followed by another convolutional layer, followed by three fully connected layers (called nn.Linear layers). The output of the final fully connected layer is a 1-D array, since we are trying to forecast 1 value: the target ENSO index."
      ],
      "metadata": {
        "id": "76GY-jWbtJaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_input_time_steps        (int) : the number of input time\n",
        "                                                steps in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.print_layer = Print()\n",
        "\n",
        "        #TIP: print out the dimension of the extracted features from\n",
        "        #the conv layers for setting the dimension of the linear layer!\n",
        "        #Using the print_layer, we find that the dimensions are\n",
        "        #(batch_size, 16, 42, 87)\n",
        "        self.fc1 = nn.Linear(16 * 42 * 87, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 1)\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        if self.print_feature_dimension:\n",
        "          x = self.print_layer(x)\n",
        "        x = x.view(-1, 16 * 42 * 87)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Print(nn.Module):\n",
        "    \"\"\"\n",
        "    This class prints out the size of the features\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "metadata": {
        "id": "sJQ7NCGstMNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(net, criterion, optimizer, trainloader, validloader, testloader,\n",
        "                  experiment_name, num_epochs=20):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      net               (nn.Module)   : the neural network architecture\n",
        "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
        "      optimizer         (torch.optim) : the optimizer to use update the neural network\n",
        "                                        to minimize the loss function\n",
        "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the train dataset\n",
        "      validloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the validation dataset\n",
        "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the test dataset\n",
        "  outputs\n",
        "  -------\n",
        "      predictions (np.array), and saves the trained neural network as a .pt file\n",
        "  \"\"\"\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  net = net.to(device)\n",
        "  best_loss = np.inf\n",
        "  train_losses, valid_losses = [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for mode, data_loader in [('train', trainloader), ('valid', validloader)]:\n",
        "      #Set the model to train mode to allow its weights to be updated\n",
        "      #while training\n",
        "      if mode == 'train':\n",
        "        net.train()\n",
        "\n",
        "      #Set the model to eval model to prevent its weights from being updated\n",
        "      #while testing\n",
        "      elif mode == 'valid':\n",
        "        net.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(data_loader):\n",
        "          # get a mini-batch of predictors and predictands\n",
        "          batch_predictors, batch_predictands = data\n",
        "          batch_predictands = batch_predictands.to(device)\n",
        "          batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          #calculate the predictions of the current neural network\n",
        "          predictions = net(batch_predictors).squeeze()\n",
        "\n",
        "          #quantify the quality of the predictions using a\n",
        "          #loss function (aka criterion) that is differentiable\n",
        "          loss = criterion(predictions, batch_predictands)\n",
        "\n",
        "          if mode == 'train':\n",
        "            #the 'backward pass: calculates the gradients of each weight\n",
        "            #of the neural network with respect to the loss\n",
        "            loss.backward()\n",
        "\n",
        "            #the optimizer updates the weights of the neural network\n",
        "            #based on the gradients calculated above and the choice\n",
        "            #of optimization algorithm\n",
        "            optimizer.step()\n",
        "\n",
        "          #Save the model weights that have the best performance!\n",
        "\n",
        "\n",
        "          running_loss += loss.item()\n",
        "      if running_loss < best_loss and mode == 'valid':\n",
        "          best_loss = running_loss\n",
        "          print(\"saving best model\")\n",
        "          torch.save(net, '{}.pt'.format(experiment_name))\n",
        "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
        "                                            running_loss/len(data_loader)))\n",
        "      if mode == 'train':\n",
        "          train_losses.append(running_loss/len(data_loader))\n",
        "      else:\n",
        "          valid_losses.append(running_loss/len(data_loader))\n",
        "\n",
        "  net = torch.load('{}.pt'.format(experiment_name), weights_only=False)\n",
        "  net.eval()\n",
        "  net.to(device)\n",
        "\n",
        "  #the remainder of this notebook calculates the predictions of the best\n",
        "  #saved model\n",
        "  predictions = np.asarray([])\n",
        "  for i, data in enumerate(testloader):\n",
        "    batch_predictors, batch_predictands = data\n",
        "    batch_predictands = batch_predictands.to(device)\n",
        "    batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "    batch_predictions = net(batch_predictors).squeeze()\n",
        "    #Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
        "    #not a Tensor. the if statement below converts it to a Tensor\n",
        "    #so that it is compatible with np.concatenate\n",
        "    if len(batch_predictions.size()) == 0:\n",
        "      batch_predictions = torch.Tensor([batch_predictions])\n",
        "    predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
        "  return predictions, train_losses, valid_losses"
      ],
      "metadata": {
        "id": "gO8quVLotYvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would happen if you removed the lines that say if mode == 'train' in the code cell below?"
      ],
      "metadata": {
        "id": "6cwSM--5tf4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assemble numpy arrays corresponding to predictors and predictands\n",
        "train_start_date = '1960-01-01'\n",
        "train_end_date = '2005-12-31'\n",
        "num_input_time_steps = 2\n",
        "lead_time = 2\n",
        "train_predictors, train_predictands = assemble_predictors_predictands(train_start_date,\n",
        "                      train_end_date, lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
        "validation_predictors, validation_predictands = assemble_predictors_predictands(train_end_date,\n",
        "                      \"2007-12-31\", lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
        "test_predictors, test_predictands = assemble_predictors_predictands('2008-01-01',\n",
        "                    '2017-12-31', lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
        "\n",
        "#Convert the numpy ararys into ENSODataset, which is a subset of the\n",
        "#torch.utils.data.Dataset class.  This class is compatible with\n",
        "#the torch dataloader, which allows for data loading for a CNN\n",
        "train_dataset = ENSODataset(train_predictors, train_predictands)\n",
        "valid_dataset = ENSODataset(validation_predictors, validation_predictands)\n",
        "test_dataset = ENSODataset(test_predictors, test_predictands)\n",
        "\n",
        "#Create a torch.utils.data.DataLoader from the ENSODatasets() created earlier!\n",
        "#the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
        "trainloader = DataLoader(train_dataset, batch_size=10)\n",
        "validloader = DataLoader(valid_dataset, batch_size=10)\n",
        "testloader = DataLoader(test_dataset, batch_size=10)\n",
        "net = CNN(num_input_time_steps=num_input_time_steps)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "experiment_name = \"twolayerCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "predictions, train_losses, test_losses = train_network(net, nn.MSELoss(),\n",
        "                  optimizer, trainloader, validloader, testloader, experiment_name)"
      ],
      "metadata": {
        "id": "oWMinCN8tivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jzVtJ9TNtpco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr, _ = pearsonr(test_predictands, predictions)\n",
        "rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
        "plot_nino_time_series(\n",
        "    test_predictands,\n",
        "    predictions,\n",
        "    '{} Predictions. Corr: {:3f}. RMSE: {:3f}.'.format(experiment_name, corr, rmse)\n",
        "    )"
      ],
      "metadata": {
        "id": "bCf8UxV9tsAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2\n",
        "\n",
        "⭐ Your turn!: Try reducing the number of parameters of the network. You could define your own network architecture, which uses a different number of parameters!"
      ],
      "metadata": {
        "id": "PaOM7rpctwcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we'll install a helpful library to sum our parameters\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "1GB3fWRztv8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# let's see how many parameters our original CNN has\n",
        "summary(net)"
      ],
      "metadata": {
        "id": "UOA5YS-fvXcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNReduced(CNN):\n",
        "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_input_time_steps        (int) : the number of input time\n",
        "                                                steps in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNNReduced, self).__init__()\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BrE_w9AQvk6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_reduced = CNNReduced(num_input_time_steps=num_input_time_steps)\n",
        "summary(net_reduced)"
      ],
      "metadata": {
        "id": "quvjF-mLvowJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net_reduced.parameters(), lr=0.0001)\n",
        "\n",
        "experiment_name = \"reducedParametersCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "predictions, train_losses, test_losses = train_network(net_reduced, nn.MSELoss(),\n",
        "                  optimizer, trainloader, testloader, experiment_name)"
      ],
      "metadata": {
        "id": "UmcAmfV3vrR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oxVeuas2vtzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr, _ = pearsonr(test_predictands, predictions)\n",
        "rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
        "plot_nino_time_series(\n",
        "    test_predictands,\n",
        "    predictions,\n",
        "    '{} Predictions. Corr: {:3f}. RMSE: {:3f}.'.format(experiment_name, corr, rmse)\n",
        "    )"
      ],
      "metadata": {
        "id": "GinFU6zOvuTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}